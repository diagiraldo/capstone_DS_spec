names(ML) <- tmp
rm(tmp)
# Fix IDs
ML$ID <- sprintf("ML%03d", as.numeric(str_extract(ML$ID, "[0-9]+$")))
# Include Split City.State string
tmp <- str_split(ML$City..State, ", ", simplify = TRUE)
ML$City <- tmp[,1]
ML$State <- ifelse(tmp[,2] %in% state_codes$state_abbr, tmp[,2], ML$State)
ML$Country <- ifelse(tmp[,2] %in% state_codes$state_abbr, "United States",
ifelse(tmp[,3] != "", tmp[,3], tmp[,2]))
# Extract km data
ML$`Total.length.(km)` <- round(as.numeric(str_extract(ML$Length.original, "^[^mi]*"))*1.60934, digits =2)
# Reorder columns
ML <- dplyr::select(ML, ID, City, State, Country, everything())
# Adjust names and merge
names(MW) <- paste("MW", names(MW), sep = ".")
MW$ID <- MW$MW.ID
names(TC) <- paste("TC", names(TC), sep = ".")
TC$ID <- TC$TC.ID
names(ML) <- paste("ML", names(ML), sep = ".")
ML$ID <- ML$ML.ID
DF <- merge(ML, TC, all = TRUE)
DF <- merge(DF, MW, all = TRUE)
# Write-Sheets
wr_url <- "https://docs.google.com/spreadsheets/d/1puOEPMcFSl9FPgjJEhLeoTpiX5I-BKZq6_IbyTfq7bQ/"
write_sheet(data = DF, ss = wr_url, sheet = "all_data")
write_sheet(data = MW, ss = wr_url, sheet = "proc_MobWorks")
write_sheet(data = TC, ss = wr_url, sheet = "proc_TabCombs")
write_sheet(data = ML, ss = wr_url, sheet = "proc_MikeLydon")
# Mobility Works data #
MW <- read_sheet(dat_url, sheet = 3, col_names = TRUE, col_types = "c")
# Adjust names
tmp <- names(MW)
tmp <- gsub("\"|\n", "", tmp)
tmp <- gsub(" ", ".", tmp)
tmp[1] <- "ID"
tmp[2] <- "internal.num"
names(MW) <- tmp
rm(tmp)
# Fix IDs
MW$ID <- sprintf("MW%03d", as.numeric(MW$internal.num))
# Detect entries with multiple countries
n_countries <- str_count(MW$Country, ", ") + 1
tmp1 <- filter(MW, n_countries > 1)
MW <- filter(MW, n_countries <= 1)
for (i in 1:nrow(tmp1)){
tmp_n <- str_count(tmp1$Country[i], ", ") + 1
tmp2 <- slice(tmp1, rep(i, tmp_n))
tmp2$Country <- as.vector(str_split(tmp1$Country[1], ", ", simplify = TRUE))
tmp2$ID <- paste(tmp2$ID, LETTERS[1:tmp_n], sep = "")
MW <- rbind(MW, tmp2)
}
i
tmp_n <- str_count(tmp1$Country[i], ", ") + 1
tmp2 <- slice(tmp1, rep(i, tmp_n))
tmp2$Country <- as.vector(str_split(tmp1$Country[1], ", ", simplify = TRUE))
as.vector(str_split(tmp1$Country[1], ", ", simplify = TRUE))
as.vector(str_split(tmp1$Country[i], ", ", simplify = TRUE))
MW <- read_sheet(dat_url, sheet = 3, col_names = TRUE, col_types = "c")
# Adjust names
tmp <- names(MW)
tmp <- gsub("\"|\n", "", tmp)
tmp <- gsub(" ", ".", tmp)
tmp[1] <- "ID"
tmp[2] <- "internal.num"
names(MW) <- tmp
rm(tmp)
# Fix IDs
MW$ID <- sprintf("MW%03d", as.numeric(MW$internal.num))
# Detect entries with multiple countries
n_countries <- str_count(MW$Country, ", ") + 1
tmp1 <- filter(MW, n_countries > 1)
MW <- filter(MW, n_countries <= 1)
for (i in 1:nrow(tmp1)){
tmp_n <- str_count(tmp1$Country[i], ", ") + 1
tmp2 <- slice(tmp1, rep(i, tmp_n))
tmp2$Country <- as.vector(str_split(tmp1$Country[i], ", ", simplify = TRUE))
tmp2$ID <- paste(tmp2$ID, LETTERS[1:tmp_n], sep = "")
MW <- rbind(MW, tmp2)
}
# Reorder columns
MW <- dplyr::select(MW, ID, City, Region, Country, everything())
# Tab Combs data #
TC <- read_sheet(dat_url, sheet = 4, col_names = TRUE, col_types = "c")
# Adjust names
tmp <- names(TC)
tmp <- gsub(" ", ".", tmp)
tmp[1] <- "internal.num"
names(TC) <- tmp
rm(tmp)
TC$ID <- sprintf("TC%03d", as.numeric(TC$internal.num))
# Include Region/State
TC <- TC %>%
mutate(Region = ifelse(Country == "USA", str_extract(City, "[A-Z]{2}$"), NA),
City = gsub(",| [A-Z]{2}$", "", City),
Country = gsub("USA", "United States", Country))
# Reorder columns
TC <- dplyr::select(TC, ID, City, Region, Country, everything())
###################
# Mike Lydon data #
###################
ML <- read_sheet(dat_url, sheet = 5, col_names = TRUE, col_types = "c",
range = "A1:Q343")
# Adjust names
tmp <- names(ML)
tmp <- gsub(",| ", ".", tmp)
tmp[1] <- "ID"
names(ML) <- tmp
rm(tmp)
# Fix IDs
ML$ID <- sprintf("ML%03d", as.numeric(str_extract(ML$ID, "[0-9]+$")))
# Include Split City.State string
tmp <- str_split(ML$City..State, ", ", simplify = TRUE)
ML$City <- tmp[,1]
ML$State <- ifelse(tmp[,2] %in% state_codes$state_abbr, tmp[,2], ML$State)
ML$Country <- ifelse(tmp[,2] %in% state_codes$state_abbr, "United States",
ifelse(tmp[,3] != "", tmp[,3], tmp[,2]))
# Extract km data
ML$`Total.length.(km)` <- round(as.numeric(str_extract(ML$Length.original, "^[^mi]*"))*1.60934, digits =2)
# Reorder columns
ML <- dplyr::select(ML, ID, City, State, Country, everything())
# Adjust names and merge
names(MW) <- paste("MW", names(MW), sep = ".")
MW$ID <- MW$MW.ID
names(TC) <- paste("TC", names(TC), sep = ".")
TC$ID <- TC$TC.ID
names(ML) <- paste("ML", names(ML), sep = ".")
ML$ID <- ML$ML.ID
DF <- merge(ML, TC, all = TRUE)
DF <- merge(DF, MW, all = TRUE)
wr_url <- "https://docs.google.com/spreadsheets/d/1puOEPMcFSl9FPgjJEhLeoTpiX5I-BKZq6_IbyTfq7bQ/"
write_sheet(data = DF, ss = wr_url, sheet = "all_data")
write_sheet(data = MW, ss = wr_url, sheet = "proc_MobWorks")
write_sheet(data = TC, ss = wr_url, sheet = "proc_TabCombs")
write_sheet(data = ML, ss = wr_url, sheet = "proc_MikeLydon")
library(dplyr)
library(ADNIMERGE)
library(corrplot)
library(RColorBrewer)
library(gtools)
library(stringr)
library (arules)
library (arulesViz)
library (devtools)
library(tidyr)
datos <- read.csv("~/ex_alejandro/datos.csv")
CN <- filter(datos, DIAGNOSIS=="CN")
CN <- select (CN,-"X", -"ORIGPROT",-"COLPROT",-"RID",-"SITEID",-"VISCODE",-"DIAGNOSIS",
-"AGE",-"PTEDUCAT", -"PTID", -"PTGENDER",-"APOE4",-"ABETA.bl",-"TAU.bl",
-"PTAU.bl",-"Q1SCORE",-"Q2SCORE",-"Q3SCORE",-"Q4SCORE",-"Q5SCORE",
-"Q6SCORE",-"Q7SCORE",-"Q8SCORE",-"Q9SCORE",-"Q10SCORE",-"Q11SCORE",
-"Q12SCORE",-"Q13SCORE", "ABETA","TAU","PTAU","APOE0","APOE1", "APOE2")
CN1 <- as.matrix(as.data.frame(lapply(CN, function (x) as.logical(x))))
CN1 <- as(CN1,"itemMatrix")
CN1 <- as(CN1,"transactions")
CN1 <- apriori(CN1, parameter= list(supp=0.4, conf=0.8, minlen =2))
ARMC<- DATAFRAME(CN1,separate = TRUE,setStart = '',itemSep = ',',setEnd = '')
###############################      DF MD         ##################################
MD <- filter(datos, DIAGNOSIS!="CN")
MD1 <- select (MD,-"X", -"ORIGPROT",-"COLPROT",-"RID",-"SITEID",-"VISCODE",-"DIAGNOSIS",
-"AGE",-"PTEDUCAT", -"PTID", -"PTGENDER",-"APOE4",-"ABETA.bl",-"TAU.bl",
-"PTAU.bl",-"Q1SCORE",-"Q2SCORE",-"Q3SCORE",-"Q4SCORE",-"Q5SCORE",
-"Q6SCORE",-"Q7SCORE",-"Q8SCORE",-"Q9SCORE",-"Q10SCORE",-"Q11SCORE",
-"Q12SCORE",-"Q13SCORE", "ABETA","TAU","PTAU","APOE0","APOE1", "APOE2")
MD1 <- as.matrix(as.data.frame(lapply(MD1, function (x) as.logical(x))))
MD1 <- as(MD1,"itemMatrix")
MD1 <- as(MD1,"transactions")
MD2 <- apriori(MD1, parameter= list(supp=0.3, conf=0.8, minlen =2))
ARMMD<- DATAFRAME(MD2,separate = TRUE,setStart = '',itemSep = ',',setEnd = '')
ARMMD$index<- as.numeric(rownames(ARMMD[,]))
### Ordenar las reglas de acuerdo a la confianza.
ARMMD<- ARMMD [order(ARMMD$confidence, decreasing= TRUE),]
rulegroups <- list()
rulegroups [[1]] <- ARMMD [1,]
for (g in 1:nrow(ARMMD)) {
items <- data.frame(RHS = NA[1:nrow(ARMMD)])
RHS2 <- data.frame () #(RHS = NA[1:nrow(ARMMD)])
#Hacer el agrupamiento del grupo x, que integra la x regla.
grupox<-data.frame(ARMMD[g,])
#Seleccionar el item consecuente de la x regla y guardarlo en el dataframe de items
x <- as.character(ARMMD [g,2])
items [1,1] <- x
#Buscar las reglas que en su lado izquierdo incluyan al item consecuente que previamente seleccionamos.
LHS <- filter(ARMMD, str_detect(ARMMD$LHS, x))
#Unir las reglas encontradas al grupo x
grupox<- rbind(grupox,LHS)
#Guardar los items consecuentes del ultimo conjunto de reglas encontradas.
RHS <- distinct (data.frame(RHS= LHS [,2]))
# la h y la k son solo para el inicio del loop.
h <- 0
k <- 0
while(nrow(RHS)>0) {
#Seleccionar el primer item consecuente, guardar el item en el dataframe de items, buscar las reglas
#que en su lado izquierdo incluyan al item consecuente seleccionado, guardar esas reglas en el grupo x,
#guardar los items consecuentes para ese conjunto de reglas.
for (i in 1:nrow(RHS)) {
x <- as.character(RHS [i,1])
items[(i+h+1),1] <- x
k<- k+1
LHS <- filter(ARMMD, str_detect(ARMMD$LHS, x))
grupox<- rbind(grupox,LHS)
RHS1 <- distinct (data.frame(RHS= LHS [,2]))
RHS2<- rbind(RHS2,RHS1)
}
#Los items consecuentes para la ultima busqueda de reglas se filtran con los items con los que ya
#se han hecho busqueda de reglas.
RHS2 <- distinct (data.frame(RHS= RHS2))
items <- na.omit(items)
for (j in 1:nrow(items)) {
RHS2 <- filter(RHS2, RHS2!= items [j,])
}
RHS <- RHS2
## la K la pongo para que con cada iteraci?n se vaya guardando la posici?n final en el dataframe items
#donde debe ir el proximo item que fue buscado.
h <- k
}
#Eliminar las reglas repetidas del grupo x y convertir el grupo a Formal Class rules
grupox<- grupox[!duplicated(grupox$index),]
#Es necesario ordenar las reglas para que al eliminar las reglas duplicadas, si dos grupos
#son iguales se filtren de la misma forma.
grupox<- grupox [order(grupox$LHS, grupox$confidence, grupox$support, decreasing = TRUE),]
grupox <- subset(x = MD2, (grupox$index))
#Eliminar reglas duplicadas del grupo x
G1<- generatingItemsets(grupox)
G1D<- which(duplicated(G1))
ARM_G1<- grupox[-G1D]
rules_Gx<- DATAFRAME(ARM_G1,separate = TRUE,setStart = '',itemSep = ',',setEnd = '')
#Eliminar reglas redundantes del grupo x
RG1<- ARM_G1[!is.redundant(ARM_G1)]
rules_Gx<- DATAFRAME(RG1,separate = TRUE,setStart = '',itemSep = ',',setEnd = '')
n<-0
for (r in 1:length(rulegroups)) {
if (nrow(semi_join(rulegroups[[r]], rules_Gx))==((nrow(rules_Gx)+nrow(rulegroups[[r]]))/2)) {
n<- n+1
}
}
f <- length(rulegroups)+1
if (n==0) {
rulegroups[[f]] <- rules_Gx
}
}
rulegroups[[1]]<
-
NUMM
rulegroups[[1]]
rulegroups[[1]]<- NULL
rulegroups[[2]]
?lapply
lapply(rulegroups, nrow)
head(rulegroups[[2]])
head(rulegroups[[3]])
head(rulegroups[[4]])
head(rulegroups[[5]])
people<- select (datos,-"X",-"APOE4",-"ABETA.bl",-"TAU.bl",
-"PTAU.bl",-"Q1SCORE",-"Q2SCORE",-"Q3SCORE",-"Q4SCORE",-"Q5SCORE",
-"Q6SCORE",-"Q7SCORE",-"Q8SCORE",-"Q9SCORE",-"Q10SCORE",-"Q11SCORE",
-"Q12SCORE",-"Q13SCORE", "ABETA","TAU","PTAU","APOE0","APOE1","APOE2")
#people = people [, c(1,8,2,9,3,7,4,5,6,10,44,45,46,47,48)]
for (k in 1:nrow(people)) {
for (h in 1:ncol(people)) {
if (people[k,h]== TRUE )
people [k,h] <- colnames(people[h])
if (people[k,h]== FALSE )
people [k,h] <- NA
}
}
people <- unite(people, items, c(11:88),  sep = ",", remove = TRUE)
transactions <- strsplit(people[,11], split=",")
# Hacer un vector de caracteres con los items de cada regla para todos los grupos
rules <- list()
for (k in 1:length(rulegroups)) {
rules[[k]]<- unite(rulegroups[[k]] , items, c(1:2),  sep = ",", remove = TRUE)
rules[[k]]<- strsplit(rules[[k]] [,1], split=",")
}
### El siguiente paso es: encontrar dentro del vector de caracteres de los items incorrectos
###  que vectores de caracteres por grupo de reglas estan incluidos en cada paciente.
for (k in 1:length(rules)) {
for (j in 1:length(transactions)) {
n<-0
for (i in 1:length(rules[[k]])){
if (length(intersect((transactions[[j]]),(rules[[k]][[i]])))==(length(rules[[k]][[i]]))) {
n<- n+1
}
}
h<- k+11
people[j,h]<- n/(length(rules[[k]]))
}
}
head(people)
sapply(rulegroups, nrow)
head(rulegroups[[2]])
head(rulegroups[[3]])
anti_join(rulegroups[[2]], rulegroups[[3]])
anti_join(rulegroups[[3]], rulegroups[[4]])
anti_join(rulegroups[[4]], rulegroups[[5]])
library(googlesheets4)
library(dplyr)
library(stringr)
library(lubridate)
# Init access to Google
gs4_auth()
# Google Sheet with merged data
dat_url = "https://docs.google.com/spreadsheets/d/1T8LE4p0-L96xiVtHsqARHiLsHsFwS8oQAT1Y2KteMXc/"
D <- read_sheet(dat_url, sheet = "new_all_data", col_names = TRUE, col_types = "c")
head(select(D, starts_with("Date")))
D$Date.announced[1]
as.Date(D$Date.announced[1])
?as.Date
as.Date(D$Date.announced[1], "%B %d, %Y")
head(D)
names(D)
D <- D %>%
mutate(Date.announced = as.Date(Date.announced, "%B %d, %Y"),
Date.started = as.Date(Date.started, "%B %d, %Y"),
Date.ended = as.Date(Date.ended, "%B %d, %Y"))
head(select(D, starts_with("Date")))
wr_url <- "https://docs.google.com/spreadsheets/d/1T8LE4p0-L96xiVtHsqARHiLsHsFwS8oQAT1Y2KteMXc/"
write_sheet(data = D, ss = wr_url, sheet = "fix_date_new_all_data")
rm(list = ls())
d <- read.table("~/tmp4view/results/group_diff_CO_AD/results_fdc/null_dist_t1.txt")
d <- read.table("~/tmp4view/results/group_diff_CO_AD/results_fdc/null_dist_t1.txt", header = FALSE)
library(R.utils)
nf <- "~/tmp4view/results/group_diff_CO_AD/results_fdc/null_dist_t1.txt"
countLines(nf)
con <- file(nf)
A <- readLines(con)
close(con)
A[1]
A[2]
strsplit()
x<-strsplit(A[2])
x<-strsplit(A[2], " ")
?strsplit
x <- as.numeric(strsplit(A[2], " ")[1])
x <- as.numeric(strsplit(A[2], " ")[[1]])
library(R.utils)
library(ggplot2)
D <- data.frame(val = as.numeric(strsplit(A[2], " ")[[1]]))
ggplot(D, aes(x = val)) + geom_histogram()
p <- ggplot(D, aes(x = val)) + theme_bw() +
geom_histogram(binwidth = 1000, fill = "orchid4", alpha = 0.5, colour = "orchid4") +
xlim(0, 40000) +
labs(x = "Enhanced statistic value", y = "Number of permutations")
p
library(dplyr)
?cumsum
?cummax
D <- data.frame(val = as.numeric(strsplit(A[2], " ")[[1]])) %>%
arrange(desc(val)) %>%
mutate(p.val = 1/nrow(D)))
D <- data.frame(val = as.numeric(strsplit(A[2], " ")[[1]])) %>%
arrange(desc(val)) %>%
mutate(p.val = 1/nrow(D))
p <- ggplot(D, aes(x = val)) + theme_bw() +
geom_histogram(binwidth = 1000, fill = "orchid4", alpha = 0.5, colour = "orchid4") +
xlim(0, 40000) +
labs(x = "Enhanced statistic value", y = "Number of permutations")
p
D <- data.frame(val = as.numeric(strsplit(A[2], " ")[[1]])) %>%
arrange(desc(val)) %>%
mutate(p.val = 1/nrow(D))
filter(D, val > 40000)
D <- data.frame(val = as.numeric(strsplit(A[2], " ")[[1]])) %>%
arrange(desc(val)) %>%
mutate(p.val = (1:nrow(D))/nrow(D))
sum(is.na(D$val))
p <- ggplot(D, aes(x = val)) + theme_bw() +
geom_histogram(binwidth = 1000, fill = "orchid4", alpha = 0.5, colour = "orchid4") +
xlim(0, 40000) +
labs(x = "Enhanced statistic value", y = "Number of permutations")
p
p <- ggplot(D, aes(x = val, y = p.val)) + theme_bw() +
geom_point(colour = "orchid4") + geom_line(colour = "orchid4") +
xlim(0, 40000) +
labs(x = "Enhanced statistic value", y = "FWE corrected p-value")
p
h <- ggplot(D, aes(x = val)) + theme_bw() +
geom_histogram(binwidth = 1000, fill = "orchid4", alpha = 0.5, colour = "orchid4") +
xlim(0, 40000) +
labs(x = "Enhanced statistic value", y = "Number of permutations") +
theme(plot.background = element_rect(fill = "transparent", colour = NA)
h
ggsave("~/Documents/proposal-defense/img/null_dist.png", h, width = 10, height = 8, units = "cm", dpi = 300, bg = "transparent")
p <- ggplot(D, aes(x = val, y = p.val)) + theme_bw() +
geom_point(colour = "orchid4") + geom_line(colour = "orchid4") +
xlim(0, 40000) +
labs(x = "Enhanced statistic value", y = "FWE corrected p-value") +
theme(plot.background = element_rect(fill = "transparent", colour = NA)
p
ggsave("~/Documents/proposal-defense/img/pvals_null_dist.png", p, width = 10, height = 8, units = "cm", dpi = 300, bg = "transparent")
h <- ggplot(D, aes(x = val)) + theme_bw() +
geom_histogram(binwidth = 1000, fill = "orchid4", alpha = 0.5, colour = "orchid4") +
xlim(0, 40000) +
labs(x = "Enhanced statistic value", y = "Number of permutations") +
theme(plot.background = element_rect(fill = "transparent", colour = NA)
h
ggsave("~/Documents/proposal-defense/img/null_distr.png", h, width = 14, height = 10, units = "cm", dpi = 300, bg = "transparent")
h <- ggplot(D, aes(x = val)) + theme_bw() +
geom_histogram(binwidth = 1000, fill = "orchid4", alpha = 0.5, colour = "orchid4") +
xlim(0, 40000) +
labs(x = "Enhanced statistic value", y = "Number of permutations") +
theme(plot.background = element_rect(fill = "transparent", colour = NA))
h
ggsave("~/Documents/proposal-defense/img/null_distr.png", h, width = 14, height = 10, units = "cm", dpi = 300, bg = "transparent")
ggsave("~/Documents/proposal-defense/img/null_distr.png", h, width = 12, height = 10, units = "cm", dpi = 300, bg = "transparent")
ggsave("~/Documents/proposal-defense/img/null_distr.png", h, width = 12, height = 8, units = "cm", dpi = 300, bg = "transparent")
p <- ggplot(D, aes(x = val, y = p.val)) + theme_bw() +
geom_point(colour = "orchid4", size = 0.5) + geom_line(colour = "orchid4") +
xlim(0, 40000) +
labs(x = "Enhanced statistic value", y = "FWE corrected p-value") +
theme(plot.background = element_rect(fill = "transparent", colour = NA))
p
ggsave("~/Documents/proposal-defense/img/pvals_null_dist.png", p, width = 10, height = 8, units = "cm", dpi = 300, bg = "transparent")
h <- ggplot(D, aes(x = val)) + theme_bw() +
geom_histogram(binwidth = 1000, fill = "orchid4", alpha = 0.5, colour = "orchid4") +
xlim(0, 40000) +
labs(x = "Enhanced statistic value", y = "Number of permutations") +
theme(plot.background = element_rect(fill = "transparent", colour = NA))
h
ggsave("~/Documents/proposal-defense/img/null_distr.png", h, width = 12, height = 8, units = "cm", dpi = 300, bg = "transparent")
p <- ggplot(D, aes(x = val, y = p.val)) + theme_bw() +
geom_point(colour = "orchid4", size = 0.5) + geom_line(colour = "orchid4") +
xlim(0, 40000) +
labs(x = "Enhanced statistic value", y = "FWE corrected p-value") +
theme(plot.background = element_rect(fill = "transparent", colour = NA))
p
ggsave("~/Documents/proposal-defense/img/pvals_null_dist.png", p, width = 12, height = 8, units = "cm", dpi = 300, bg = "transparent")
h <- ggplot(D, aes(x = val)) + theme_bw() +
geom_histogram(binwidth = 1000, fill = "orchid4", alpha = 0.5, colour = "orchid4") +
xlim(0, 40000) +
labs(x = "Enhanced statistic value", y = "Number of permutations") +
theme(plot.background = element_rect(fill = "transparent", colour = NA))
h
ggsave("~/Documents/proposal-defense/img/null_distr.png", h, width = 10, height = 8, units = "cm", dpi = 300, bg = "transparent")
p <- ggplot(D, aes(x = val, y = p.val)) + theme_bw() +
geom_point(colour = "orchid4", size = 0.5) + geom_line(colour = "orchid4") +
xlim(0, 40000) +
labs(x = "Enhanced statistic value", y = "FWE corrected p-value") +
theme(plot.background = element_rect(fill = "transparent", colour = NA))
p
ggsave("~/Documents/proposal-defense/img/pvals_null_dist.png", p, width = 10, height = 8, units = "cm", dpi = 300, bg = "transparent")
h <- ggplot(D, aes(x = val)) + theme_bw() +
geom_histogram(binwidth = 1000, fill = "orchid4", alpha = 0.5, colour = "orchid4") +
xlim(0, 40000) +
labs(x = "Max. enhanced statistic value", y = "Number of permutations") +
theme(plot.background = element_rect(fill = "transparent", colour = NA))
h
ggsave("~/Documents/proposal-defense/img/null_distr.png", h, width = 10, height = 8, units = "cm", dpi = 300, bg = "transparent")
p <- ggplot(D, aes(x = val, y = p.val)) + theme_bw() +
geom_point(colour = "orchid4", size = 0.5) + geom_line(colour = "orchid4") +
xlim(0, 40000) +
labs(x = "Max. enhanced statistic value", y = "FWE corrected p-value") +
theme(plot.background = element_rect(fill = "transparent", colour = NA))
p
ggsave("~/Documents/proposal-defense/img/pvals_null_dist.png", p, width = 10, height = 8, units = "cm", dpi = 300, bg = "transparent")
library(ADNIMERGE)
??`ADNIMERGE-package`
??mri
??dtiroi
?dtiroi
?mriprot
head(mriprot)
library(dplyr); A <- filter(mriprot, COLPROT =="ADNI3")
table(mriserial$COLPROT)
?mri3gometa
A <- filter(mri3gometa, COLPROT =="ADNI3")
table(mri3gometa$COLPROT)
table(mri3meta$COLPROT)
A <- read.csv("~/Downloads/MRI3META.csv")
head(A)
table(A$COLPROT)
table(A$PHASE)
A <- filter(A, PHASE == "ADNI3")
head(A)
?A
4387000-2084000
4387000-2084000-100000
?markovchainFit
library("markovchain")
library("markovchain")
short_text <- c("the quick brown fox jumps over the lazy dog and the angry dog chase the fox")
text_term <- strsplit(short_text, split = " ") %>% unlist()
fit_markov <- markovchainFit(text_term, method = "laplace")
library("dplyr")
text_term <- strsplit(short_text, split = " ") %>% unlist()
fit_markov <- markovchainFit(text_term, method = "laplace")
markovchainSequence(n = 4, markovchain = fit_markov$estimate, t0 = "the", include.t0 = T)
markovchainSequence(n = 4, markovchain = fit_markov$estimate, t0 = "dog", include.t0 = T)
knitr::opts_chunk$set(echo = TRUE)
setwd("~/capstone_DS_spec/")
library("R.utils")
library("tidytext")
library("dplyr")
library("ggplot2")
knitr::opts_chunk$set(echo = TRUE, message = FALSE)
setwd("~/capstone_DS_spec/")
library("R.utils")
library("tidytext")
library("dplyr")
library("ggplot2")
TS <- data.frame(source = c("news", "blogs", "twitter"),
nlines = NA, av.words = NA)
s<-1
infile <- sprintf("data/en_US/en_US.%s.txt", TS$source[s])
TS$nlines[s] <- countLines(infile)
con <- file(infile)
tmp <- readLines(con)
close(con)
sapply(strsplit(tmp, "\\s+"), length)
?kable
?mean
s
